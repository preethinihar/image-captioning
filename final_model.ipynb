{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2e1f3d-b820-433f-b926-67c63b75f072",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "import os\n",
    "import csv\n",
    "import string\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB0, preprocess_input\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# =========================\n",
    "# CONFIGURATION\n",
    "# =========================\n",
    "DATA_DIR = '/content/drive/MyDrive/archive (5)/Images'\n",
    "CAPTION_FILE = '/content/drive/MyDrive/archive (5)/captions.txt'\n",
    "\n",
    "VOCAB_SIZE = 10000\n",
    "MAX_LENGTH = 40\n",
    "EMBEDDING_DIM = 256\n",
    "LSTM_UNITS = 256\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "\n",
    "# =========================\n",
    "# LOAD & CLEAN CAPTIONS\n",
    "# =========================\n",
    "\n",
    "\n",
    "def load_captions(caption_file):\n",
    "    captions_dict = defaultdict(list)\n",
    "    with open(caption_file, 'r') as f:\n",
    "        reader = csv.DictReader(f)  # Handles the header: image,caption\n",
    "        for row in reader:\n",
    "            img_id = row['image'].strip()\n",
    "            caption = row['caption'].strip().lower()\n",
    "            caption = caption.translate(str.maketrans('', '', string.punctuation))\n",
    "            caption = 'startseq ' + caption + ' endseq'\n",
    "            captions_dict[img_id].append(caption)\n",
    "    return captions_dict\n",
    "\n",
    "# Debug: See how many images match\n",
    "all_captioned_images = set(captions_dict.keys())\n",
    "existing_images = set(os.listdir(DATA_DIR))\n",
    "matched = all_captioned_images & existing_images\n",
    "\n",
    "print(f\"Total images in captions.txt: {len(all_captioned_images)}\")\n",
    "print(f\"Total images in folder: {len(existing_images)}\")\n",
    "print(f\"Matched image-caption pairs: {len(matched)}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# EXTRACT FEATURES\n",
    "# =========================\n",
    "def extract_image_features(image_dir, image_list):\n",
    "    model = EfficientNetB0(weights='imagenet', include_top=False, pooling='avg')\n",
    "    features = {}\n",
    "    for img_name in tqdm(image_list):\n",
    "        img_path = os.path.join(image_dir, img_name)\n",
    "        if not os.path.exists(img_path):\n",
    "            continue\n",
    "        try:\n",
    "            img = image.load_img(img_path, target_size=(224, 224))\n",
    "            img_array = image.img_to_array(img)\n",
    "            img_array = np.expand_dims(img_array, axis=0)\n",
    "            img_array = preprocess_input(img_array)\n",
    "            feature = model.predict(img_array, verbose=0)\n",
    "            features[img_name] = feature\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {img_name}: {e}\")\n",
    "    return features\n",
    "\n",
    "# =========================\n",
    "# TOKENIZER\n",
    "# =========================\n",
    "def create_tokenizer(captions, vocab_size):\n",
    "    tokenizer = Tokenizer(num_words=vocab_size, oov_token='<unk>', filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "    tokenizer.fit_on_texts(captions)\n",
    "    return tokenizer\n",
    "\n",
    "# =========================\n",
    "# DATA GENERATOR\n",
    "# =========================\n",
    "def data_generator(image_features, captions_dict, tokenizer, max_length, vocab_size):\n",
    "    for img_id, captions in captions_dict.items():\n",
    "        if img_id not in image_features:\n",
    "            continue\n",
    "        for caption in captions:\n",
    "            seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "            for i in range(1, len(seq)):\n",
    "                in_seq, out_seq = seq[:i], seq[i]\n",
    "                in_seq = pad_sequences([in_seq], maxlen=max_length, padding='post')[0]\n",
    "                out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                img_feature = image_features[img_id][0]\n",
    "                yield (img_feature, in_seq), out_seq\n",
    "\n",
    "# =========================\n",
    "# BUILD MODEL\n",
    "# =========================\n",
    "def build_decoder(vocab_size, max_length, embedding_dim, units):\n",
    "    inputs1 = Input(shape=(1280,))\n",
    "    fe1 = Dropout(0.4)(inputs1)\n",
    "    fe2 = Dense(units, activation='relu')(fe1)\n",
    "\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.4)(se1)\n",
    "    se3 = LSTM(units)(se2)\n",
    "\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(units, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# =========================\n",
    "# GENERATE CAPTION\n",
    "# =========================\n",
    "def generate_caption(model, tokenizer, photo, max_length):\n",
    "    in_text = 'startseq'\n",
    "    for _ in range(max_length):\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length, padding='post')\n",
    "        yhat = model.predict([photo, sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = tokenizer.index_word.get(yhat)\n",
    "        if word is None or word == 'endseq':\n",
    "            break\n",
    "        in_text += ' ' + word\n",
    "    return in_text.replace('startseq', '').strip()\n",
    "\n",
    "# =========================\n",
    "# MAIN\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading captions...\")\n",
    "    captions_dict = load_captions(CAPTION_FILE)\n",
    "\n",
    "    image_list = [img for img in captions_dict.keys() if os.path.exists(os.path.join(DATA_DIR, img))]\n",
    "    print(f\"Number of valid images: {len(image_list)}\")\n",
    "\n",
    "    print(\"Extracting image features...\")\n",
    "    image_features = extract_image_features(DATA_DIR, image_list)\n",
    "\n",
    "    print(\"Preparing tokenizer...\")\n",
    "    all_captions = [caption for caps in captions_dict.values() for caption in caps]\n",
    "    tokenizer = create_tokenizer(all_captions, VOCAB_SIZE)\n",
    "\n",
    "    print(\"Building model...\")\n",
    "    model = build_decoder(VOCAB_SIZE, MAX_LENGTH, EMBEDDING_DIM, LSTM_UNITS)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    print(\"Preparing dataset...\")\n",
    "    total_captions = sum(len(v) for v in captions_dict.values())\n",
    "    steps_per_epoch = total_captions // BATCH_SIZE\n",
    "    print(f\"Total captions: {total_captions}, Steps per epoch: {steps_per_epoch}\")\n",
    "\n",
    "    if steps_per_epoch == 0:\n",
    "        raise ValueError(\"Not enough data to train. Check if images and captions match.\")\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: data_generator(image_features, captions_dict, tokenizer, MAX_LENGTH, VOCAB_SIZE),\n",
    "        output_signature=(\n",
    "            (\n",
    "                tf.TensorSpec(shape=(1280,), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(MAX_LENGTH,), dtype=tf.int32)\n",
    "            ),\n",
    "            tf.TensorSpec(shape=(VOCAB_SIZE,), dtype=tf.float32)\n",
    "        )\n",
    "    ).repeat().shuffle(2048).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    print(\"Training model...\")\n",
    "    model.fit(dataset, epochs=EPOCHS, steps_per_epoch=steps_per_epoch)\n",
    "\n",
    "    print(\"\\nGenerating caption for test image...\")\n",
    "    test_img = list(image_features.keys())[0]\n",
    "    test_feat = image_features[test_img]\n",
    "    caption = generate_caption(model, tokenizer, test_feat, MAX_LENGTH)\n",
    "\n",
    "    img_path = os.path.join(DATA_DIR, test_img)\n",
    "    img = Image.open(img_path)\n",
    "\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title(caption)\n",
    "    plt.show()\n",
    "pip install nltk rouge-score\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "def evaluate_model(model, captions_dict, image_features, tokenizer, max_length):\n",
    "    from nltk.translate.bleu_score import corpus_bleu\n",
    "    from nltk.translate.meteor_score import meteor_score\n",
    "    from rouge_score import rouge_scorer\n",
    "    import numpy as np\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "\n",
    "    rouge = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    meteor_scores, rouge1_scores, rougeL_scores = [], [], []\n",
    "\n",
    "    for img_id, refs in tqdm(captions_dict.items(), desc=\"Evaluating\"):\n",
    "        if img_id not in image_features:\n",
    "            continue\n",
    "        photo = image_features[img_id]\n",
    "        pred = generate_caption(model, tokenizer, photo, max_length)\n",
    "\n",
    "        tokenized_refs = [r.split() for r in refs]\n",
    "        tokenized_pred = pred.split()\n",
    "\n",
    "        references.append(tokenized_refs)\n",
    "        hypotheses.append(tokenized_pred)\n",
    "\n",
    "        meteor_scores.append(meteor_score(tokenized_refs, tokenized_pred))\n",
    "        rouge_scores = rouge.score(refs[0], pred)\n",
    "        rouge1_scores.append(rouge_scores['rouge1'].fmeasure)\n",
    "        rougeL_scores.append(rouge_scores['rougeL'].fmeasure)\n",
    "\n",
    "    print(\"\\nEvaluation Metrics:\")\n",
    "    print(\"BLEU-1:\", corpus_bleu(references, hypotheses, weights=(1.0, 0, 0, 0)))\n",
    "    print(\"BLEU-2:\", corpus_bleu(references, hypotheses, weights=(0.5, 0.5, 0, 0)))\n",
    "    print(\"BLEU-3:\", corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0)))\n",
    "    print(\"BLEU-4:\", corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "    print(\"METEOR:\", np.mean(meteor_scores))\n",
    "    print(\"ROUGE-1:\", np.mean(rouge1_scores))\n",
    "    print(\"ROUGE-L:\", np.mean(rougeL_scores))\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # Optional but often needed\n",
    "evaluate_model(model, captions_dict, image_features, tokenizer, MAX_LENGTH)\n",
    "pip install streamlit\n",
    "!pip install streamlit\n",
    "!pip install pyngrok\n",
    "!pip install streamlit pyngrok tensorflow pillow\n",
    "!pip install gradio\n",
    "!pip install gradio\n",
    "import gradio as gr\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Load the pretrained BLIP model and processor\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# Caption generation function\n",
    "def generate_caption(image):\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    out = model.generate(**inputs)\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "# Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=generate_caption,\n",
    "    inputs=gr.Image(type=\"pil\"),\n",
    "    outputs=gr.Textbox(),\n",
    ")\n",
    "\n",
    "iface.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
